import torch
import torch.optim as optim
from stubs.pytorch.layers import *
import math
import numpy as np
import torch.nn.functional as F
from torch.nn.parameter import Parameter

class SingleLayer(BasicUnit):
  def __init__(self, blocks, algorithm="nad", beta=None, reduction=False):
    super(SingleLayer, self).__init__()
    self.reduction = reduction
    self.algorithm = algorithm

    if not self.reduction and self.algorithm == "final_arch":
      self.block = blocks[0]
      return

    self.blocks = nn.ModuleList(blocks)
    self.AP_path_alpha = Parameter(torch.ones(len(self.blocks)))  # architecture parameters
    self.AP_path_wb = Parameter(torch.Tensor(len(self.blocks)))  # binary gates
    #self.grads = []

    if self.algorithm == "plnas" and reduction == False:
      self.indexes = np.sort(np.random.choice(range(len(self.blocks)),
          size=(2), replace=False))
      self.prev_epochs = 0

    elif self.algorithm == "nad" and reduction == False:
      self.prev_epochs = torch.tensor(0).cuda()
      self.beta = torch.tensor(beta.item()).cuda()
      self.policy_p1 = torch.tensor(1.).cuda()
      self.sampled_freq = torch.zeros(size=(len(blocks),)).cuda()
      self.n_runs=0

    elif self.algorithm == "final_arch" and reduction == False:
      self.block = self.blocks[0]
    self.lambda_ = torch.tensor(0.1).cuda()

  def pad_image(self, x, padding_type, kernel_size, stride=1, dilation=1):
    if padding_type == 'SAME':
        p0 = ((x.shape[2] - 1) * (stride - 1) + dilation * (kernel_size[0] - 1))# //2
        p1 = ((x.shape[3] - 1) * (stride - 1) + dilation * (kernel_size[1] - 1))# //2
        #print(x.shape, kernel_size, p0, p1)
        input_rows = x.shape[2]
        filter_rows = kernel_size[0]

        x = F.pad(x, [0, p1, 0, p0])
        return x

  def pad_image_v2(self, x, padding_type, kernel_size, stride=1, dilation=1):
    if padding_type == 'SAME':
      effective_kernel_size_0 = (kernel_size[0] - 1) * dilation + 1
      out_dim = (x.shape[2] + stride - 1) // stride
      p0 = max(0, (out_dim - 1) * stride + effective_kernel_size_0 - x.shape[2])

      effective_kernel_size_1 = (kernel_size[1] - 1) * dilation + 1
      out_dim = (x.shape[3] + stride - 1) // stride
      p1 = max(0, (out_dim - 1) * stride + effective_kernel_size_1 - x.shape[3])

      padding_before_0 = p0 // 2
      padding_before_1 = p1 // 2

      padding_after_0 = p0 - padding_before_0
      padding_after_1 = p1 - padding_before_1

      x = F.pad(x, [padding_before_1, padding_after_1, padding_before_0, padding_after_0])
      return x

  def get_indexes_random(self):
    indexes = np.sort(np.random.choice(range(len(self.blocks)),
          size=(2), replace=False))
    return indexes

  def get_indexes_darts(self):
    pass

  def get_indexes_plnas(self):
    self.epoch += 1
    probs = []
    for block in self.blocks:
      probs.append(block.alpha)
    probs = torch.stack(probs)
    sm_probs = F.softmax(probs, dim=0)
    self.indexes = indexes = torch.sort(torch.multinomial(sm_probs, 2, replacement=False))[0]
    return self.indexes

  def get_indexes_snas(self):
    pass

  def get_indexes_nad(self):
    """ Write this function's explanation here """

    #if self.phase == "test":
     # probs = F.softmax(self.AP_path_alpha / self.lambda_, dim=0)
      #indexes = torch.multinomial(probs.data, 2, replacement=False)
      #torch.gather might be the speed bottleneck here
      #todo compare perf. with probs[indexes[0/1]]
      #probs_slice = F.softmax(torch.stack([
       #         self.AP_path_alpha[idx] for idx in indexes
        #    ]), dim=0)
      #c = torch.multinomial(probs_slice.data, 1)[0]
    #  return indexes, None

    #print("EPOCH=", self.epoch, "PREV_EPOCH=", self.prev_epochs)
    #if self.prev_epochs <= self.epoch:
     # self.policy_p1 *= self.beta
     # print("Beta= ", self.beta)
     # self.prev_epochs += 1
    #print("POLICY P1=", self.policy_p1)
    #policy = torch.bernoulli(self.policy_p1).item()
    #print("POLICY=", policy)
    #if policy == 0:
      # Exploration policy
      # sm_self.AP_path_alpha = F.softmax(self.AP_path_alpha, dim=0)
      #probs = F.softmax(-1 * self.sampled_freq, dim=0)
      #probs = torch.clamp(probs, min=0.01, max=0.96)
      #print(probs)
      #if self.n_runs < len(self.AP_path_alpha) :
        #index = self.n_runs
      #else:
      #probs = F.softmax(self.AP_path_alpha.data + (torch.sqrt(2*torch.log(float(self.n_runs+1)/self.sampled_freq))),dim=0)
      #indexes = torch.multinomial(probs.data, 2, replacement=False)
      #print("ALPHA BEFORE UCB =", self.AP_path_alpha.data)
      #print("FREQUENCIES",self.sampled_freq)
      #print("NRUNS=", self.n_runs+1)
      #print("DIVIDEND", float(self.n_runs+1)/self.sampled_freq)
      #print("LOG DIVIDEND", torch.log(float(self.n_runs+1)/self.sampled_freq))
      #print("CONFIDENCE BOUND=", (torch.sqrt(2*torch.log(float(self.n_runs+1)/self.sampled_freq))))
      #print("CB SIZE", (torch.sqrt(2*torch.log(float(self.n_runs+1)/self.sampled_freq))).size())
      #print("SORTED VALUES=", values)
      #print("INDEXES=", indexes)
      #index = indexes[0]
      #print("INDEX=", index)
      #indexes = torch.sort(torch.multinomial(probs, 2, replacement=False))[0]
      #new_probs = F.softmax(torch.gather(sm_self.AP_path_alpha, 0, indexes), dim=0)
      #if phase == "train":
        # If the phase is search_arch then we should not increase the sampling freq
      #index = indexes[0]
      #self.sampled_freq[indexes] += 1
     # self.n_runs += 1
    #else:
      # exploitation policy
    probs = F.softmax(self.AP_path_alpha / self.lambda_, dim=0)
      #probs = torch.clamp(probs, min=0.01, max=0.96)
      #print(probs) 
    indexes = torch.multinomial(probs.data, 2, replacement=False)
      #probs_slice = F.softmax(torch.stack([
                #self.AP_path_alpha[idx] for idx in indexes
            #]), dim=0)
      #print("PROBS SLICE= ",probs_slice)
      #index = torch.multinomial(probs_slice.data, 1)[0]
      # new_probs = F.softmax(torch.gather(probs, 0, indexes), dim=0, )
      #if phase == "train":
    print("INDEXES AFTER EXPLOIT", indexes, "ALPHAS=", self.AP_path_alpha[indexes])
      #index = indexes[0]
      #self.sampled_freq[index] += 1
    return indexes, None

  def apply_reduction(self, x):
    outputs = []

    for block in self.blocks:
      outputs.append(block(x))
    concat_output = torch.cat(outputs, dim=1)

    return concat_output

  def reset_binary_gates(self):
    #reset all gates
    self.grads = []
    self.AP_path_wb.data.zero_()
    #probs = F.softmax(self.AP_path_alpha / self.lambda_, dim=0)
    #print(probs)
    #print(probs.data)
    if self.epoch<75:
      c = 1.414
    else:
      self.policy_p1 *= self.beta
      c = self.policy_p1
    probs, indexes = F.softmax(self.AP_path_alpha.data + c*(torch.sqrt(torch.log(float(self.n_runs))/self.sampled_freq)))
    print("PROBS=", probs, "INDEXES=", indexes)
    #probs = F.softmax(self.AP_path_alpha / self.lambda_, dim=0)
    #values, indexes = torch.sort(probs.data) #torch.multinomial(probs.data, 2, replacement=False)
    sample_op = [indexes[0], indexes[1]]
    active_op = sample_op[0]
    inactive_op = sample_op[1]
    #sample_op = torch.multinomial(probs.data, 2, replacement=False)
    #probs_slice = F.softmax(torch.stack([
     #           self.AP_path_alpha[idx] for idx in sample_op
      #      ]), dim=0)
    #c = torch.multinomial(probs_slice.data, 1)[0]
    #active_op = sample_op[c].item()
    #inactive_op = sample_op[1 - c].item()
    self.active_index = [active_op]
    self.inactive_index = [inactive_op]
    # set binary gate
    self.n_runs += 1
    self.sampled_freq[active_op] += 1
    self.AP_path_wb.data[active_op] = 1.0
    self.AP_path_wb.register_hook(lambda grad:self.grads.append(grad.clone()))
    #print("GRADS OF ALL PATHS=", self.grads)

  def forward(self, x):
    if self.reduction:
      output = self.apply_reduction(x)
      return output

    else:
      if self.algorithm == "final_arch":
        k = self.block.kernel_size
        x = self.pad_image_v2(x, padding_type='SAME', kernel_size=k)
        x = self.block(x)
        return x

      elif self.algorithm == "plnas":
        # ProxylessNAS sampling
        indexes = self.get_indexes_plnas()
        new_probs = F.softmax(torch.gather(probs, 0, indexes), dim=0)

      elif self.algorithm == "nad" and self.phase is not 'validation':
        # Neural Arch. Design sampling
        indexes, new_probs = self.get_indexes_nad()

      elif self.algorithm == "snas":
        indexes = self.get_indexes_snas()

      elif self.algorithm == "darts":
        indexes = self.get_indexes_darts()

      elif self.algorithm == "random":
        # Random sampling
        indexes = self.get_indexes_random()

      output = 0

      if self.phase == "validation":
        a_i = self.active_index[0]
        in_i= self.inactive_index[0]
        k0 = self.blocks[a_i].kernel_size
        k1 = self.blocks[in_i].kernel_size
        x0 = self.pad_image(x, padding_type='SAME', kernel_size=k0)
        x1 = self.pad_image(x, padding_type='SAME', kernel_size=k1)
        oi = self.blocks[a_i](x0)
        output = output + (self.AP_path_wb[a_i] * oi)
        oi = self.blocks[in_i](x1)
        output = output + (self.AP_path_wb[in_i] * oi.detach())
	      
      else:
        k0 = self.blocks[indexes[0]].kernel_size
